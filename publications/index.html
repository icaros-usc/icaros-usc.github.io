<!doctype html><html lang="en"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,shrink-to-fit=no"><title>Publications | ICAROS Lab</title><link rel="canonical" href="https://icaros.usc.edu/publications/"><link rel="alternate" type="application/atom+xml" title="ICAROS Lab Blog" href="/feed.xml"><meta name="description" content="Publications by the ICAROS lab."><meta property="og:title" content="Publications"><meta property="og:type" content="website"><meta property="og:url" content="https://icaros.usc.edu/publications/"><meta property="og:image" content="https://icaros.usc.edu/imgs/open-graph-preview.png"><meta property="og:description" content="Publications by the ICAROS lab."><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Publications"><meta name="twitter:description" content="Publications by the ICAROS lab."><meta name="twitter:image" content="https://icaros.usc.edu/imgs/twitter-preview.png"><link rel="stylesheet" href="/assets/main.css?e0cde93547a51a1d3c0c4e5bff99c64a"><link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.1/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css"><link rel="apple-touch-icon" sizes="180x180" href="/favicon/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/favicon/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/favicon/favicon-16x16.png"><link rel="manifest" href="/favicon/site.webmanifest"><link rel="mask-icon" href="/favicon/safari-pinned-tab.svg" color="#000000"><link rel="shortcut icon" href="/favicon/favicon.ico"><meta name="msapplication-TileColor" content="#000000"><meta name="msapplication-config" content="/favicon/browserconfig.xml"><meta name="theme-color" content="#ffffff"></head><body class="flex flex-col min-h-screen"><div style="min-height:80vh"><header><nav class="w-full bg-white relative" style="box-shadow:0 2px 4px rgba(0,0,0,.2)"><div class="flex items-center flex-wrap justify-between max-w-screen-2xl md:px-4 mx-auto p-0"><div class="w-full flex flex-wrap items-center justify-between md:justify-start md:w-auto"><div class="border-b-4 border-black pb-0.5 transition border-opacity-0 hover:border-opacity-100 z-50 md:ml-1 md:my-0 ml-5 mr-2 my-4" style="padding-top:4px"><a href="/"><img src="/imgs/logo/black.svg" class="h-10 inline-block" alt="ICAROS Lab" loading="lazy"></a></div><input class="cursor-pointer absolute right-0 top-0 md:hidden mr-5 mt-5 nav-checkbox opacity-0 z-40" style="width:32px;height:28px" type="checkbox"><div class="md:hidden mr-4 my-4 nav-hamburger"><div id="hamburger" class="cursor-pointer duration-300 ease mx-2 relative transition-all" style="width:26px;height:22px"><div class="duration-300 ease transition-all absolute bg-black hamburger-bar bar1" style="top:0;width:26px;height:2px"></div><div class="duration-300 ease transition-all absolute bg-black hamburger-bar bar2" style="top:10px;width:26px;height:2px"></div><div class="duration-300 ease transition-all absolute bg-black hamburger-bar bar3" style="top:20px;width:26px;height:2px"></div></div></div><style>.nav-checkbox:checked~.nav-hamburger .bar1{transform:translateY(10px) rotate(45deg);transition:all .2s ease}.nav-checkbox:checked~.nav-hamburger .bar2{opacity:0;transition:all .2s ease}.nav-checkbox:checked~.nav-hamburger .bar3{transform:translateY(-10px) rotate(-45deg);transition:all .2s ease}</style><div class="w-full md:w-auto hidden md:flex md:py-0 md:text-left nav-menu pb-3 text-center"><div class="mt-3 block mb-1 md:inline-block md:my-6 mx-3"><a href="/research/" class="text-black border-b-4 border-black hover:no-underline pb-0.5 pt-1 transition border-opacity-0 hover:border-opacity-100 hover:text-black">Research</a></div><div class="mt-3 block mb-1 md:inline-block md:my-6 mx-3"><a href="/people/" class="text-black border-b-4 border-black hover:no-underline pb-0.5 pt-1 transition border-opacity-0 hover:border-opacity-100 hover:text-black">People</a></div><div class="mt-3 block mb-1 md:inline-block md:my-6 mx-3"><a href="/publications/" class="text-black border-b-4 border-black hover:no-underline pb-0.5 pt-1 transition hover:border-gray-400 hover:text-gray-400">Publications</a></div><div class="mt-3 block mb-1 md:inline-block md:my-6 mx-3"><a href="/press/" class="text-black border-b-4 border-black hover:no-underline pb-0.5 pt-1 transition border-opacity-0 hover:border-opacity-100 hover:text-black">Press</a></div><div class="mt-3 block mb-1 md:inline-block md:my-6 mx-3"><a href="/videos/" class="text-black border-b-4 border-black hover:no-underline pb-0.5 pt-1 transition border-opacity-0 hover:border-opacity-100 hover:text-black">Videos</a></div></div><style>.nav-checkbox:checked~.nav-hamburger~.nav-menu{display:block}.group-checkbox:checked~.group-menu{display:block}</style></div><div class="hidden border-b-4 border-black border-opacity-0 hover:border-opacity-100 lg:mr-5 md:block my-0 pb-0.5 transition z-50" style="padding-top:4px"><a href="https://usc.edu"><img src="/imgs/usc.png" class="h-10 inline-block" alt="USC" loading="lazy"></a></div></div></nav></header><main class="flex-grow"><div class="mx-auto __publications__ max-w-4xl p-4"><h2 class="font-light mt-6 text-4xl">2021</h2><div class="w-full"><h3 class="mt-3 mb-2 text-2xl">Conferences</h3><div class="w-full flex flex-wrap border-gray-100 border-t-2 hover:bg-gray-100 px-2 py-3"><div class="w-full"><a href="https://arxiv.org/abs/2106.10853" class="text-black font-bold hover:text-primary">On the Importance of Environments in Human-Robot Coordination</a><p class="text-gray-500 text-base">Matthew C. Fontaine*, Ya-Chuan Hsu*, Yulun Zhang*, Bryon Tjakana, Stefanos Nikolaidis</p></div><p class="text-gray-500 italic text-sm w-5/6">Robotics: Science and Systems, July 2021</p><div class="w-full my-1"><a class="mr-3" href="https://arxiv.org/abs/2106.10853"><span class="pr-0.5 text-xl"><i class="ai ai-arxiv"></i></span> <span class="">arXiv</span> </a><a class="mr-3" href="https://overcooked-lsi.github.io/"><span class="pr-0.5 text-lg"><i class="fas fa-globe-americas"></i></span> <span class="">Website</span></a></div><div class="w-full"><details><summary class="pl-0.5 cursor-pointer hover:text-primary hover:underline"><i class="far fa-file-alt"></i> <span class="pl-0.5">Abstract</span></summary><div class="px-4 py-2"><p>When studying robots collaborating with humans, much of the focus has been on robot policies that coordinate fluently with human teammates in collaborative tasks. However, less emphasis has been placed on the effect of the environment on coordination behaviors. To thoroughly explore environments that result in diverse behaviors, we propose a framework for procedural generation of environments that are (1) stylistically similar to human-authored environments, (2) guaranteed to be solvable by the human-robot team, and (3) diverse with respect to coordination measures. We analyze the procedurally generated environments in the Overcooked benchmark domain via simulation and an online user study. Results show that the environments result in qualitatively different emerging behaviors and statistically significant differences in collaborative fluency metrics, even when the robot runs the same planning algorithm.</p></div></details></div><div class="w-full"><details><summary class="pl-0.5 cursor-pointer hover:text-primary hover:underline"><i class="fas fa-quote-right"></i> <span class="pl-0.5">Citation</span></summary><div class="px-4 py-2"><div class="relative"><div id="clipboard-0"><pre class="language-tex"><code class="language-tex">@inproceedings{fontaine2021importance,
  author = {Fontaine*, Matthew C. and Hsu*, Ya-Chuan and Zhang*, Yulun and Tjakana, Bryon and Nikolaidis, Stefanos},
  booktitle = {Proceedings of {Robotics}: Science and {Systems}},
  year = {2021},
  month = {7},
  title = {On the {Importance} of {Environments} in {Human}-{Robot} {Coordination}},
}
</code></pre></div><button class="flex items-center absolute clipboard cursor-pointer focus:bg-gray-800 focus:bg-opacity-50 focus:outline-none focus:text-white group hover:bg-gray-800 hover:bg-opacity-50 hover:text-white p-2 right-0 text-gray-400 text-xs top-0 transition" data-clipboard-target="#clipboard-0"><span class="hidden pr-2 group-focus:hidden group-hover:inline-block">Copy </span><span class="hidden pr-2 group-focus:inline-block group-hover:hidden">Copied! </span><span class="material-icons">content_copy</span></button></div></div></details></div></div><div class="w-full flex flex-wrap border-gray-100 border-t-2 hover:bg-gray-100 px-2 py-3"><div class="w-full"><a href="https://arxiv.org/abs/2012.04283" class="text-black font-bold hover:text-primary">A Quality Diversity Approach to Automatically Generating Human-Robot Interaction Scenarios in Shared Autonomy</a><p class="text-gray-500 text-base">Matthew C. Fontaine, Stefanos Nikolaidis</p></div><p class="text-gray-500 italic text-sm w-5/6">Proceedings of Robotics: Science and Systems, July 2021</p><div class="w-full my-1"><a class="mr-3" href="https://arxiv.org/abs/2012.04283"><span class="pr-0.5 text-xl"><i class="ai ai-arxiv"></i></span> <span class="">arXiv</span></a></div><div class="w-full"><details><summary class="pl-0.5 cursor-pointer hover:text-primary hover:underline"><i class="far fa-file-alt"></i> <span class="pl-0.5">Abstract</span></summary><div class="px-4 py-2"><p>The growth of scale and complexity of interactions between humans and robots highlights the need for new computational methods to automatically evaluate novel algorithms and applications. Exploring diverse scenarios of humans and robots interacting in simulation can improve understanding of the robotic system and avoid potentially costly failures in real-world settings. We formulate this problem as a quality diversity (QD) problem, where the goal is to discover diverse failure scenarios by simultaneously exploring both environments and human actions. We focus on the shared autonomy domain, where the robot attempts to infer the goal of a human operator, and adopt the QD algorithm MAP-Elites to generate scenarios for two published algorithms in this domain: shared autonomy via hindsight optimization and linear policy blending. Some of the generated scenarios confirm previous theoretical findings, while others are surprising and bring about a new understanding of state-of-the-art implementations. Our experiments show that MAP-Elites outperforms Monte-Carlo simulation and optimization based methods in effectively searching the scenario space, highlighting its promise for automatic evaluation of algorithms in human-robot interaction.</p></div></details></div><div class="w-full"><details><summary class="pl-0.5 cursor-pointer hover:text-primary hover:underline"><i class="fas fa-quote-right"></i> <span class="pl-0.5">Citation</span></summary><div class="px-4 py-2"><div class="relative"><div id="clipboard-1"><pre class="language-tex"><code class="language-tex">@inproceedings{fontaine2021shared,
  author = {Fontaine, Matthew C. and Nikolaidis, Stefanos},
  booktitle = {Proceedings of {Robotics}: Science and {Systems}},
  year = {2021},
  month = {7},
  title = {A {Quality} {Diversity} {Approach} to {Automatically} {Generating} {Human}-{Robot} {Interaction} {Scenarios} in {Shared} {Autonomy}},
}
</code></pre></div><button class="flex items-center absolute clipboard cursor-pointer focus:bg-gray-800 focus:bg-opacity-50 focus:outline-none focus:text-white group hover:bg-gray-800 hover:bg-opacity-50 hover:text-white p-2 right-0 text-gray-400 text-xs top-0 transition" data-clipboard-target="#clipboard-1"><span class="hidden pr-2 group-focus:hidden group-hover:inline-block">Copy </span><span class="hidden pr-2 group-focus:inline-block group-hover:hidden">Copied! </span><span class="material-icons">content_copy</span></button></div></div></details></div></div><div class="w-full flex flex-wrap border-gray-100 border-t-2 hover:bg-gray-100 px-2 py-3"><div class="w-full"><a href="https://cs.brown.edu/~gdk/pubs/push_grasp_clutter.pdf" class="text-black font-bold hover:text-primary">Learning Collaborative Pushing and Grasping Policies in Dense Clutter</a><p class="text-gray-500 text-base">Bingjie Tang, Matthew Corsaro, George Konidaris, Stefanos Nikolaidis, Stefanie Tellex</p></div><p class="text-gray-500 italic text-sm w-5/6">2021 IEEE International Conference on Robotics and Automation (ICRA), May 2021</p><div class="w-full my-1"><a class="mr-3" href="https://cs.brown.edu/~gdk/pubs/push_grasp_clutter.pdf"><span class="pr-0.5 text-lg"><i class="far fa-file-pdf"></i></span> <span class="">PDF</span></a></div><div class="w-full"><details><summary class="pl-0.5 cursor-pointer hover:text-primary hover:underline"><i class="far fa-file-alt"></i> <span class="pl-0.5">Abstract</span></summary><div class="px-4 py-2"><p>Robots must reason about pushing and grasping in order to engage in flexible manipulation in cluttered environments. Earlier works on learning pushing and grasping only consider each operation in isolation or are limited to top-down grasping and bin-picking. We train a robot to learn joint planar pushing and 6-degree-of-freedom (6-DoF) grasping policies by self-supervision. Two separate deep neural networks are trained to map from 3D visual observations to actions with a Q-learning framework. With collaborative pushes and expanded grasping action space, our system can deal with cluttered scenes with a wide variety of objects (e.g. grasping a plate from the side after pushing away surrounding obstacles). We compare our system to the state-of-the-art baseline model VPG in simulation and outperform it with 10% higher action efficiency and 20% higher grasp success rate. We then demonstrate our system on a KUKA LBR iiwa arm with a Robotiq 3-finger gripper.</p></div></details></div><div class="w-full"><details><summary class="pl-0.5 cursor-pointer hover:text-primary hover:underline"><i class="fas fa-quote-right"></i> <span class="pl-0.5">Citation</span></summary><div class="px-4 py-2"><div class="relative"><div id="clipboard-2"><pre class="language-tex"><code class="language-tex">@article{tang2021learning,
  author = {Tang, Bingjie and Corsaro, Matthew and Konidaris, George and Nikolaidis, Stefanos and Tellex, Stefanie},
  journal = {2021 IEEE International Conference on Robotics and Automation (ICRA)},
  year = {2021},
  month = {5},
  title = {Learning {Collaborative} {Pushing} and {Grasping} {Policies} in {Dense} {Clutter}},
}
</code></pre></div><button class="flex items-center absolute clipboard cursor-pointer focus:bg-gray-800 focus:bg-opacity-50 focus:outline-none focus:text-white group hover:bg-gray-800 hover:bg-opacity-50 hover:text-white p-2 right-0 text-gray-400 text-xs top-0 transition" data-clipboard-target="#clipboard-2"><span class="hidden pr-2 group-focus:hidden group-hover:inline-block">Copy </span><span class="hidden pr-2 group-focus:inline-block group-hover:hidden">Copied! </span><span class="material-icons">content_copy</span></button></div></div></details></div></div><div class="w-full flex flex-wrap border-gray-100 border-t-2 hover:bg-gray-100 px-2 py-3"><div class="w-full"><a href="https://arxiv.org/abs/2103.14994" class="text-black font-bold hover:text-primary">Two-Stage Clustering of Human Preferences for Action Prediction in Assembly Tasks</a><p class="text-gray-500 text-base">Heramb Nemlekar, Jignesh Modi, Satyandra K. Gupta, Stefanos Nikolaidis</p></div><p class="text-gray-500 italic text-sm w-5/6">2021 IEEE International Conference on Robotics and Automation (ICRA), May 2021</p><div class="w-full my-1"><a class="mr-3" href="https://arxiv.org/abs/2103.14994"><span class="pr-0.5 text-xl"><i class="ai ai-arxiv"></i></span> <span class="">arXiv</span></a></div><div class="w-full"><details><summary class="pl-0.5 cursor-pointer hover:text-primary hover:underline"><i class="far fa-file-alt"></i> <span class="pl-0.5">Abstract</span></summary><div class="px-4 py-2"><p>To effectively assist human workers in assembly tasks a robot must proactively offer support by inferring their preferences in sequencing the task actions. Previous work has focused on learning the dominant preferences of human workers for simple tasks largely based on their intended goal. However, people may have preferences at different resolutions: they may share the same high-level preference for the order of the sub-tasks but differ in the sequence of individual actions. We propose a two-stage approach for learning and inferring the preferences of human operators based on the sequence of sub-tasks and actions. We conduct an IKEA assembly study and demonstrate how our approach is able to learn the dominant preferences in a complex task. We show that our approach improves the prediction of human actions through cross-validation. Lastly, we show that our two-stage approach improves the efficiency of task execution in an online experiment, and demonstrate its applicability in a real-world robot-assisted IKEA assembly.</p></div></details></div><div class="w-full"><details><summary class="pl-0.5 cursor-pointer hover:text-primary hover:underline"><i class="fas fa-quote-right"></i> <span class="pl-0.5">Citation</span></summary><div class="px-4 py-2"><div class="relative"><div id="clipboard-3"><pre class="language-tex"><code class="language-tex">@article{nemlekar2021twostage,
  author = {Nemlekar, Heramb and Modi, Jignesh and Gupta, Satyandra K. and Nikolaidis, Stefanos},
  journal = {2021 IEEE International Conference on Robotics and Automation (ICRA)},
  year = {2021},
  month = {5},
  title = {Two-{Stage} {Clustering} of {Human} {Preferences} for {Action} {Prediction} in {Assembly} {Tasks}},
}
</code></pre></div><button class="flex items-center absolute clipboard cursor-pointer focus:bg-gray-800 focus:bg-opacity-50 focus:outline-none focus:text-white group hover:bg-gray-800 hover:bg-opacity-50 hover:text-white p-2 right-0 text-gray-400 text-xs top-0 transition" data-clipboard-target="#clipboard-3"><span class="hidden pr-2 group-focus:hidden group-hover:inline-block">Copy </span><span class="hidden pr-2 group-focus:inline-block group-hover:hidden">Copied! </span><span class="material-icons">content_copy</span></button></div></div></details></div></div><div class="w-full flex flex-wrap border-gray-100 border-t-2 hover:bg-gray-100 px-2 py-3"><div class="w-full"><a href="https://arxiv.org/abs/2007.05674" class="text-black font-bold hover:text-primary">Illuminating Mario Scenes in the Latent Space of a Generative Adversarial Network</a><p class="text-gray-500 text-base">Matthew C. Fontaine, Ruilin Liu, Ahmed Khalifa, Jignesh Modi, Julian Togelius, Amy K. Hoover, Stefanos Nikolaidis</p></div><p class="text-gray-500 italic text-sm w-5/6">AAAI Conference on Artificial Intelligence, February 2021</p><div class="w-full my-1"><a class="mr-3" href="https://arxiv.org/abs/2007.05674"><span class="pr-0.5 text-xl"><i class="ai ai-arxiv"></i></span> <span class="">arXiv</span> </a><a class="mr-3" href="https://github.com/icaros-usc/MarioGAN-LSI"><span class="pr-0.5 text-lg"><i class="fab fa-github"></i></span> <span class="">GitHub</span></a></div><div class="w-full"><details><summary class="pl-0.5 cursor-pointer hover:text-primary hover:underline"><i class="far fa-file-alt"></i> <span class="pl-0.5">Abstract</span></summary><div class="px-4 py-2"><p>Generative adversarial networks (GANs) are quickly becoming a ubiquitous approach to procedurally generating video game levels. While GAN generated levels are stylistically similar to human-authored examples, human designers often want to explore the generative design space of GANs to extract interesting levels. However, human designers find latent vectors opaque and would rather explore along dimensions the designer specifies, such as number of enemies or obstacles. We propose using state-of-the-art quality diversity algorithms designed to optimize continuous spaces, i.e. MAP-Elites with a directional variation operator and Covariance Matrix Adaptation MAP-Elites, to efficiently explore the latent space of a GAN to extract levels that vary across a set of specified gameplay measures. In the benchmark domain of Super Mario Bros, we demonstrate how designers may specify gameplay measures to our system and extract high-quality (playable) levels with a diverse range of level mechanics, while still maintaining stylistic similarity to human authored examples. An online user study shows how the different mechanics of the automatically generated levels affect subjective ratings of their perceived difficulty and appearance.</p></div></details></div><div class="w-full"><details><summary class="pl-0.5 cursor-pointer hover:text-primary hover:underline"><i class="fas fa-quote-right"></i> <span class="pl-0.5">Citation</span></summary><div class="px-4 py-2"><div class="relative"><div id="clipboard-4"><pre class="language-tex"><code class="language-tex">@article{fontaine2021illuminating,
  author = {Fontaine, Matthew C. and Liu, Ruilin and Khalifa, Ahmed and Modi, Jignesh and Togelius, Julian and Hoover, Amy K. and Nikolaidis, Stefanos},
  journal = {AAAI Conference on Artificial Intelligence},
  year = {2021},
  month = {2},
  title = {Illuminating {Mario} {Scenes} in the {Latent} {Space} of a {Generative} {Adversarial} {Network}},
  volume = {35},
}
</code></pre></div><button class="flex items-center absolute clipboard cursor-pointer focus:bg-gray-800 focus:bg-opacity-50 focus:outline-none focus:text-white group hover:bg-gray-800 hover:bg-opacity-50 hover:text-white p-2 right-0 text-gray-400 text-xs top-0 transition" data-clipboard-target="#clipboard-4"><span class="hidden pr-2 group-focus:hidden group-hover:inline-block">Copy </span><span class="hidden pr-2 group-focus:inline-block group-hover:hidden">Copied! </span><span class="material-icons">content_copy</span></button></div></div></details></div></div></div><div class="w-full"><h3 class="mt-3 mb-2 text-2xl">Preprints</h3><div class="w-full flex flex-wrap border-gray-100 border-t-2 hover:bg-gray-100 px-2 py-3"><div class="w-full"><a href="https://arxiv.org/abs/2106.03894" class="text-black font-bold hover:text-primary">Differentiable Quality Diversity</a><p class="text-gray-500 text-base">Matthew C. Fontaine, Stefanos Nikolaidis</p></div><p class="text-gray-500 italic text-sm w-5/6">CoRR, June 2021</p><div class="w-full my-1"><a class="mr-3" href="https://arxiv.org/abs/2106.03894"><span class="pr-0.5 text-xl"><i class="ai ai-arxiv"></i></span> <span class="">arXiv</span> </a><a class="mr-3" href="https://github.com/icaros-usc/dqd"><span class="pr-0.5 text-lg"><i class="fab fa-github"></i></span> <span class="">GitHub</span></a></div><div class="w-full"><details><summary class="pl-0.5 cursor-pointer hover:text-primary hover:underline"><i class="far fa-file-alt"></i> <span class="pl-0.5">Abstract</span></summary><div class="px-4 py-2"><p>Quality diversity (QD) is a growing branch of stochastic optimization research that studies the problem of generating an archive of solutions that maximize a given objective function but are also diverse with respect to a set of specified measure functions. However, even when these functions are differentiable, QD algorithms treat them as "black boxes", ignoring gradient information. We present the differentiable quality diversity (DQD) problem, a special case of QD, where both the objective and measure functions are first order differentiable. We then present MAP-Elites via Gradient Arborescence (MEGA), a DQD algorithm that leverages gradient information to efficiently explore the joint range of the objective and measure functions. Results in two QD benchmark domains and in searching the latent space of a StyleGAN show that MEGA significantly outperforms state-of-the-art QD algorithms, highlighting DQD's promise for efficient quality diversity optimization when gradient information is available.</p></div></details></div><div class="w-full"><details><summary class="pl-0.5 cursor-pointer hover:text-primary hover:underline"><i class="fas fa-quote-right"></i> <span class="pl-0.5">Citation</span></summary><div class="px-4 py-2"><div class="relative"><div id="clipboard-5"><pre class="language-tex"><code class="language-tex">@article{fontaine2021differentiable,
  author = {Fontaine, Matthew C. and Nikolaidis, Stefanos},
  journal = {CoRR},
  year = {2021},
  month = {6},
  title = {Differentiable {Quality} {Diversity}},
  volume = {abs/2012.04283},
}
</code></pre></div><button class="flex items-center absolute clipboard cursor-pointer focus:bg-gray-800 focus:bg-opacity-50 focus:outline-none focus:text-white group hover:bg-gray-800 hover:bg-opacity-50 hover:text-white p-2 right-0 text-gray-400 text-xs top-0 transition" data-clipboard-target="#clipboard-5"><span class="hidden pr-2 group-focus:hidden group-hover:inline-block">Copy </span><span class="hidden pr-2 group-focus:inline-block group-hover:hidden">Copied! </span><span class="material-icons">content_copy</span></button></div></div></details></div></div></div><h2 class="font-light mt-6 text-4xl">2020</h2></div></main></div><div class="flex-grow"></div><footer><div class="w-full text-center bg-gray-100 p-6"><div class="mx-auto max-w-screen-2xl"><div class="flex flex-wrap justify-center mx-auto"><a href="https://github.com/icaros-usc/" title="GitHub (icaros-usc)" class="flex items-center bg-white h-12 hover:bg-gray-200 hover:no-underline hover:text-primary-dark justify-center mx-1 rounded-full text-2xl text-primary w-12"><i class="mx-auto fab fa-github"></i> </a><a href="https://www.youtube.com/channel/UCUuIwdCu3yXZBv-KtPS5TvQ" title="YouTube" class="flex items-center bg-white h-12 hover:bg-gray-200 hover:no-underline hover:text-primary-dark justify-center mx-1 rounded-full text-2xl text-primary w-12"><i class="mx-auto fab fa-youtube"></i> </a><a href="https://twitter.com/icaroslab" title="Twitter (@icaroslab)" class="flex items-center bg-white h-12 hover:bg-gray-200 hover:no-underline hover:text-primary-dark justify-center mx-1 rounded-full text-2xl text-primary w-12"><i class="mx-auto fab fa-twitter"></i> </a><a href="mailto:nikolaid@usc.edu" title="Email (nikolaid@usc.edu)" class="flex items-center bg-white h-12 hover:bg-gray-200 hover:no-underline hover:text-primary-dark justify-center mx-1 rounded-full text-2xl text-primary w-12"><i class="fas fa-envelope mx-auto"></i></a></div><div class="w-full flex flex-wrap justify-center md:flex-nowrap pt-3 text-black"><p class="w-full md:w-auto md:order-1 order-3">© ICAROS Lab 2021</p><p class="hidden md:inline px-1 order-2">|</p><p class="w-full md:w-auto md:order-3 order-5">In memory of <a href="https://www.linkedin.com/in/jignesh-modi-7819a8b8">Jignesh</a></p><p class="hidden md:inline px-1 order-4">|</p><p class="w-full md:w-auto mb-1 md:mb-0 md:order-5 order-1">See anything wrong? <a href="https://github.com/icaros-usc/icaros-usc.github.io/issues/new/">Raise an Issue</a></p></div></div></div></footer><script src="/assets/main.js?e49202d1526e360b0fe7399dd607d56b"></script><script src="/assets/vendor.js?40d6fac326e5360678edc8ddd7f519b8"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.6/dist/clipboard.min.js"></script><script>new ClipboardJS(".clipboard")</script></body></html>