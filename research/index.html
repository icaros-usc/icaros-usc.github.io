<!doctype html><html lang="en"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,shrink-to-fit=no"><title>Research | ICAROS</title><link rel="canonical" href="https://icaros.usc.edu/research/"><link rel="alternate" type="application/atom+xml" title="ICAROS Lab Blog" href="/feed.xml"><meta name="description" content="An overview of our research areas and contributions."><meta property="og:title" content="Research"><meta property="og:type" content="website"><meta property="og:url" content="https://icaros.usc.edu/research/"><meta property="og:image" content="https://icaros.usc.edu/imgs/open-graph-preview.png"><meta property="og:description" content="An overview of our research areas and contributions."><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Research"><meta name="twitter:description" content="An overview of our research areas and contributions."><meta name="twitter:image" content="https://icaros.usc.edu/imgs/twitter-preview.png"><link rel="stylesheet" href="/assets/main.css?7ed226ee987a3d0ca7dccaf09f56185c"><link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.1/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css"><link rel="apple-touch-icon" sizes="180x180" href="/favicon/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/favicon/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/favicon/favicon-16x16.png"><link rel="manifest" href="/favicon/site.webmanifest"><link rel="mask-icon" href="/favicon/safari-pinned-tab.svg" color="#000000"><link rel="shortcut icon" href="/favicon/favicon.ico"><meta name="msapplication-TileColor" content="#000000"><meta name="msapplication-config" content="/favicon/browserconfig.xml"><meta name="theme-color" content="#ffffff"></head><body class="flex flex-col min-h-screen"><div style="min-height:80vh"><header><nav class="w-full relative bg-white z-40" style="box-shadow:0 2px 4px rgba(0,0,0,.2)"><div class="flex items-center flex-wrap justify-between max-w-screen-2xl md:px-4 mx-auto p-0"><div class="w-full md:w-auto flex flex-wrap items-center justify-between md:justify-start"><div class="border-b-4 border-black pb-0.5 transition border-opacity-0 hover:border-opacity-100 z-50 md:ml-1 md:my-0 ml-5 mr-2 my-4" style="padding-top:4px"><a href="/"><img src="/imgs/logo/black.svg" class="h-10 inline-block" alt="ICAROS" loading="lazy"></a></div><input class="cursor-pointer absolute right-0 top-0 md:hidden mr-5 mt-5 nav-checkbox opacity-0 z-50" style="width:32px;height:28px" type="checkbox"><div class="md:hidden mr-4 my-4 nav-hamburger"><div id="hamburger" class="cursor-pointer duration-300 ease mx-2 relative transition-all" style="width:26px;height:22px"><div class="duration-300 ease transition-all absolute bg-black hamburger-bar bar1" style="top:0;width:26px;height:2px"></div><div class="duration-300 ease transition-all absolute bg-black hamburger-bar bar2" style="top:10px;width:26px;height:2px"></div><div class="duration-300 ease transition-all absolute bg-black hamburger-bar bar3" style="top:20px;width:26px;height:2px"></div></div></div><style>.nav-checkbox:checked~.nav-hamburger .bar1{transform:translateY(10px) rotate(45deg);transition:all .2s ease}.nav-checkbox:checked~.nav-hamburger .bar2{opacity:0;transition:all .2s ease}.nav-checkbox:checked~.nav-hamburger .bar3{transform:translateY(-10px) rotate(-45deg);transition:all .2s ease}</style><div class="w-full md:w-auto hidden md:flex md:py-0 md:text-left nav-menu pb-3 text-center"><div class="block mb-1 md:inline-block md:my-6 mt-3 mx-3"><a href="/research/" class="text-black border-b-4 border-black hover:no-underline pb-0.5 pt-1 transition hover:border-gray-400 hover:text-gray-400">Research</a></div><div class="block mb-1 md:inline-block md:my-6 mt-3 mx-3"><a href="/people/" class="text-black border-b-4 border-black hover:no-underline pb-0.5 pt-1 transition border-opacity-0 hover:border-opacity-100 hover:text-black">People</a></div><div class="block mb-1 md:inline-block md:my-6 mt-3 mx-3"><a href="/publications/" class="text-black border-b-4 border-black hover:no-underline pb-0.5 pt-1 transition border-opacity-0 hover:border-opacity-100 hover:text-black">Publications</a></div><div class="block mb-1 md:inline-block md:my-6 mt-3 mx-3"><a href="/press/" class="text-black border-b-4 border-black hover:no-underline pb-0.5 pt-1 transition border-opacity-0 hover:border-opacity-100 hover:text-black">Press</a></div><div class="block mb-1 md:inline-block md:my-6 mt-3 mx-3"><a href="/videos/" class="text-black border-b-4 border-black hover:no-underline pb-0.5 pt-1 transition border-opacity-0 hover:border-opacity-100 hover:text-black">Videos</a></div></div><style>.nav-checkbox:checked~.nav-hamburger~.nav-menu{display:block}.group-checkbox:checked~.group-menu{display:block}</style></div><div class="hidden border-b-4 border-black border-opacity-0 hover:border-opacity-100 lg:mr-5 md:block my-0 pb-0.5 transition z-50" style="padding-top:4px"><a href="https://usc.edu"><img src="/imgs/usc.png" class="h-10 inline-block" alt="USC" loading="lazy"></a></div></div></nav></header><main class="flex-grow"><div class="px-4 __research__ max-w-screen-xl mx-auto py-8"><div class="max-w-none prose"><h1>Research Overview</h1><p>Our lab focuses on enabling robots to interact robustly with users in unconstrained and dynamic environments. We draw upon insights from artificial intelligence, human-robot interaction, procedural content generation, and quality diversity optimization to make fundamental advances in both developing interactive robots that assist users in complex, real-world tasks, as well as in generating complex, diverse, and realistic scenarios that effectively test the developed systems to enhance their robustness.</p></div><div class="max-w-none prose mb-4 mt-8"><h2 id="research-contribution-%231%3A-robot-assistance-in-human-environments">Research Contribution #1: Robot Assistance in Human Environments <a class="permalink" href="#research-contribution-%231%3A-robot-assistance-in-human-environments">¶</a></h2></div><div class="flex items-center flex-wrap"><div class="w-full lg:w-7/12 max-w-none prose"><p>In order for robots to be effective assistants in human environments, they need to be able to adapt to the human user, based on the user’s physical characteristics, their individualized preferences, and their priorities on how to execute the task. For example, a robot that helps a post-stroke patient with hair combing should recognize and follow the user’s hair style. A manufacturing robot that supports human workers in assembly manufacturing should anticipate which tool a worker is going to need next. While much prior work has focused on learning from demonstration, in many such tasks human demonstrations are tedious, time-consuming, or impractical.</p><p>We have designed algorithms that efficiently infer the the user’s physical and mental states. This is made possible with very little data by using compact representations of these states and prior knowledge in the form of dominant user preferences and signal temporal logic. Our models have enabled general-purpose robotic arms to proactively assist users in a variety of tasks, such as IKEA assembly, hair combing, and meal preparation.</p></div><div class="w-full lg:pl-4 lg:w-5/12 py-4"><div class="w-full relative h-0" style="padding-bottom:56.25%"><iframe title="Need help building IKEA furniture? This robot learns your preferences and lends a hand." class="w-full absolute h-full left-0 top-0" src="https://www.youtube.com/embed/wDHQXBua4OI" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div></div></div><div class="mt-4"><div class="flex flex-wrap"><div class="w-full lg:w-1/2"><div class="w-full border-gray-100 border-t-2 hover:bg-gray-100 p-3"><a href="https://arxiv.org/abs/2108.01233" class="text-black block font-bold hover:text-primary">Design and Evaluation of a Hair Combing System Using a General-Purpose Robotic Arm</a><p class="text-gray-600 text-base">Nathaniel Dennler, Eura Shin, Maja Matarić, Stefanos Nikolaidis</p><p class="text-gray-600 italic text-sm">2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), October 2021</p><div class="my-1"><a class="mr-3" href="https://arxiv.org/abs/2108.01233"><span class="pr-0.5 text-xl"><i class="ai ai-arxiv"></i></span> <span class="">arXiv</span></a></div><div><details><summary class="pl-0.5 cursor-pointer hover:text-primary hover:underline"><i class="far fa-file-alt"></i> <span class="pl-0.5">Abstract</span></summary><div class="px-4 py-2"><p>This work introduces an approach for automatic hair combing by a lightweight robot. For people living with limited mobility, dexterity, or chronic fatigue, combing hair is often a difficult task that negatively impacts personal routines. We propose a modular system for enabling general robot manipulators to assist with a hair-combing task. The system consists of three main components. The first component is the segmentation module, which segments the location of hair in space. The second component is the path planning module that proposes automatically-generated paths through hair based on user input. The final component creates a trajectory for the robot to execute. We quantitatively evaluate the effectiveness of the paths planned by the system with 48 users and qualitatively evaluate the system with 30 users watching videos of the robot performing a hair-combing task in the physical world. The system is shown to effectively comb different hairstyles.</p></div></details></div><div><details><summary class="pl-0.5 cursor-pointer hover:text-primary hover:underline"><i class="fas fa-quote-right"></i> <span class="pl-0.5">Citation</span></summary><div class="px-4 py-2"><div class="relative"><div id="clipboard-44"><pre class="language-tex"><code class="language-tex">@inproceedings{dennler2021combing,
  author={Dennler, Nathaniel and Shin, Eura and Matarić, Maja and Nikolaidis, Stefanos},
  booktitle={2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  title={Design and Evaluation of a Hair Combing System Using a General-Purpose Robotic Arm},
  year={2021},
  month={October},
  abstract={This work introduces an approach for automatic hair combing by a lightweight robot. For people living with limited mobility, dexterity, or chronic fatigue, combing hair is often a difficult task that negatively impacts personal routines. We propose a modular system for enabling general robot manipulators to assist with a hair-combing task. The system consists of three main components. The first component is the segmentation module, which segments the location of hair in space. The second component is the path planning module that proposes automatically-generated paths through hair based on user input. The final component creates a trajectory for the robot to execute. We quantitatively evaluate the effectiveness of the paths planned by the system with 48 users and qualitatively evaluate the system with 30 users watching videos of the robot performing a hair-combing task in the physical world. The system is shown to effectively comb different hairstyles.},
}
</code></pre></div><button class="cursor-pointer absolute right-0 top-0 clipboard flex focus:bg-gray-800 focus:bg-opacity-50 focus:outline-none focus:text-white group hover:bg-gray-800 hover:bg-opacity-50 hover:text-white items-center p-2 text-gray-400 text-xs transition" data-clipboard-target="#clipboard-44"><span class="hidden pr-2 group-focus:hidden group-hover:inline-block">Copy </span><span class="hidden pr-2 group-focus:inline-block group-hover:hidden">Copied! </span><span class="material-icons">content_copy</span></button></div></div></details></div></div></div><div class="w-full lg:w-1/2"><div class="w-full border-gray-100 border-t-2 hover:bg-gray-100 p-3"><a href="https://arxiv.org/abs/2103.14994" class="text-black block font-bold hover:text-primary">Two-Stage Clustering of Human Preferences for Action Prediction in Assembly Tasks</a><p class="text-gray-600 text-base">Heramb Nemlekar, Jignesh Modi, Satyandra K. Gupta, Stefanos Nikolaidis</p><p class="text-gray-600 italic text-sm">2021 IEEE International Conference on Robotics and Automation (ICRA), May 2021</p><div class="my-1"><a class="mr-3" href="https://arxiv.org/abs/2103.14994"><span class="pr-0.5 text-xl"><i class="ai ai-arxiv"></i></span> <span class="">arXiv</span></a></div><div><details><summary class="pl-0.5 cursor-pointer hover:text-primary hover:underline"><i class="far fa-file-alt"></i> <span class="pl-0.5">Abstract</span></summary><div class="px-4 py-2"><p>To effectively assist human workers in assembly tasks a robot must proactively offer support by inferring their preferences in sequencing the task actions. Previous work has focused on learning the dominant preferences of human workers for simple tasks largely based on their intended goal. However, people may have preferences at different resolutions: they may share the same high-level preference for the order of the sub-tasks but differ in the sequence of individual actions. We propose a two-stage approach for learning and inferring the preferences of human operators based on the sequence of sub-tasks and actions. We conduct an IKEA assembly study and demonstrate how our approach is able to learn the dominant preferences in a complex task. We show that our approach improves the prediction of human actions through cross-validation. Lastly, we show that our two-stage approach improves the efficiency of task execution in an online experiment, and demonstrate its applicability in a real-world robot-assisted IKEA assembly.</p></div></details></div><div><details><summary class="pl-0.5 cursor-pointer hover:text-primary hover:underline"><i class="fas fa-quote-right"></i> <span class="pl-0.5">Citation</span></summary><div class="px-4 py-2"><div class="relative"><div id="clipboard-45"><pre class="language-tex"><code class="language-tex">@article{nemlekar2021twostage,
  title={Two-Stage Clustering of Human Preferences for Action Prediction in
         Assembly Tasks},
  author={Heramb Nemlekar and Jignesh Modi and Satyandra K. Gupta and Stefanos Nikolaidis},
  journal={2021 IEEE International Conference on Robotics and Automation (ICRA)},
  year={2021},
  month={May},
  url={https://arxiv.org/abs/2103.14994},
  abstract={To effectively assist human workers in assembly tasks a robot must proactively offer support by inferring their preferences in sequencing the task actions. Previous work has focused on learning the dominant preferences of human workers for simple tasks largely based on their intended goal. However, people may have preferences at different resolutions: they may share the same high-level preference for the order of the sub-tasks but differ in the sequence of individual actions. We propose a two-stage approach for learning and inferring the preferences of human operators based on the sequence of sub-tasks and actions. We conduct an IKEA assembly study and demonstrate how our approach is able to learn the dominant preferences in a complex task. We show that our approach improves the prediction of human actions through cross-validation. Lastly, we show that our two-stage approach improves the efficiency of task execution in an online experiment, and demonstrate its applicability in a real-world robot-assisted IKEA assembly.}
}
</code></pre></div><button class="cursor-pointer absolute right-0 top-0 clipboard flex focus:bg-gray-800 focus:bg-opacity-50 focus:outline-none focus:text-white group hover:bg-gray-800 hover:bg-opacity-50 hover:text-white items-center p-2 text-gray-400 text-xs transition" data-clipboard-target="#clipboard-45"><span class="hidden pr-2 group-focus:hidden group-hover:inline-block">Copy </span><span class="hidden pr-2 group-focus:inline-block group-hover:hidden">Copied! </span><span class="material-icons">content_copy</span></button></div></div></details></div></div></div><div class="w-full lg:w-1/2"><div class="w-full border-gray-100 border-t-2 hover:bg-gray-100 p-3"><a href="https://ieeexplore.ieee.org/document/9515466" class="text-black block font-bold hover:text-primary">Personalizing User Engagement Dynamics in a Non-Verbal Communication Game for Cerebral Palsy</a><p class="text-gray-600 text-base">Nathaniel Dennler, Catherine Yunis, Jonathan Realmuto, Terence Sanger, Stefanos Nikolaidis, Maja Matarić</p><p class="text-gray-600 italic text-sm">2021 30th IEEE International Conference on Robot Human Interactive Communication (RO-MAN), 2021</p><div class="my-1"><a class="mr-3" href="https://ieeexplore.ieee.org/document/9515466"><span class="pr-0.5 text-lg"><i class="ai ai-ieee"></i></span> <span class="">IEEE Xplore</span> </a><a class="mr-3" href="https://arxiv.org/abs/2107.07446"><span class="pr-0.5 text-xl"><i class="ai ai-arxiv"></i></span> <span class="">arXiv</span></a></div><div><details><summary class="pl-0.5 cursor-pointer hover:text-primary hover:underline"><i class="far fa-file-alt"></i> <span class="pl-0.5">Abstract</span></summary><div class="px-4 py-2"><p>Children and adults with cerebral palsy (CP) can have involuntary upper limb movements as a consequence of the symptoms that characterize their motor disability, leading to difficulties in communicating with caretakers and peers. We describe how a socially assistive robot may help individuals with CP to practice non-verbal communicative gestures using an active orthosis in a one-on-one number-guessing game. We performed a user study and data collection with participants with CP; we found that participants preferred an embodied robot over a screen-based agent, and we used the participant data to train personalized models of participant engagement dynamics that can be used to select personalized robot actions. Our work highlights the benefit of personalized models in the engagement of users with CP with a socially assistive robot and offers design insights for future work in this area.</p></div></details></div><div><details><summary class="pl-0.5 cursor-pointer hover:text-primary hover:underline"><i class="fas fa-quote-right"></i> <span class="pl-0.5">Citation</span></summary><div class="px-4 py-2"><div class="relative"><div id="clipboard-46"><pre class="language-tex"><code class="language-tex">@inproceedings{dennler2021personalizing,
    author={Dennler, Nathaniel and Yunis, Catherine and Realmuto, Jonathan and Sanger, Terence and Nikolaidis, Stefanos and Matarić, Maja},
    booktitle={2021 30th IEEE International Conference on Robot Human Interactive Communication (RO-MAN)},
     title={Personalizing User Engagement Dynamics in a Non-Verbal Communication Game for Cerebral Palsy},
    year={2021},
    pages={873-879},
    doi={10.1109/RO-MAN50785.2021.9515466},
    abstract={Children and adults with cerebral palsy (CP) can have involuntary upper limb movements as a consequence of the symptoms that characterize their motor disability, leading to difficulties in communicating with caretakers and peers. We describe how a socially assistive robot may help individuals with CP to practice non-verbal communicative gestures using an active orthosis in a one-on-one number-guessing game. We performed a user study and data collection with participants with CP; we found that participants preferred an embodied robot over a screen-based agent, and we used the participant data to train personalized models of participant engagement dynamics that can be used to select personalized robot actions. Our work highlights the benefit of personalized models in the engagement of users with CP with a socially assistive robot and offers design insights for future work in this area.}
}
</code></pre></div><button class="cursor-pointer absolute right-0 top-0 clipboard flex focus:bg-gray-800 focus:bg-opacity-50 focus:outline-none focus:text-white group hover:bg-gray-800 hover:bg-opacity-50 hover:text-white items-center p-2 text-gray-400 text-xs transition" data-clipboard-target="#clipboard-46"><span class="hidden pr-2 group-focus:hidden group-hover:inline-block">Copy </span><span class="hidden pr-2 group-focus:inline-block group-hover:hidden">Copied! </span><span class="material-icons">content_copy</span></button></div></div></details></div></div></div><div class="w-full lg:w-1/2"><div class="w-full border-gray-100 border-t-2 hover:bg-gray-100 p-3"><a href="https://corlconf.github.io/corl2020/paper_498/" class="text-black block font-bold hover:text-primary">Learning from Demonstrations using Signal Temporal Logic</a><p class="text-gray-600 text-base">Aniruddh Puranic, Jyotirmoy Deshmukh, Stefanos Nikolaidis</p><p class="text-gray-600 italic text-sm">Conference on Robot Learning, November 2020</p><div class="my-1"><a class="mr-3" href="https://corlconf.github.io/corl2020/paper_498/"><span class="pr-0.5 text-lg"><i class="fas fa-book"></i></span> <span class="">CoRL</span> </a><a class="mr-3" href="https://arxiv.org/abs/2102.07730"><span class="pr-0.5 text-xl"><i class="ai ai-arxiv"></i></span> <span class="">arXiv</span> </a><a class="mr-3" href="https://www.youtube.com/watch?v=tCzu2fKb45s"><span class="pr-0.5 text-lg"><i class="fab fa-youtube"></i></span> <span class="">YouTube</span></a></div><div><details><summary class="pl-0.5 cursor-pointer hover:text-primary hover:underline"><i class="far fa-file-alt"></i> <span class="pl-0.5">Abstract</span></summary><div class="px-4 py-2"><p>We present a model-based reinforcement learning framework for robot locomotion that achieves walking based on only 4.5 minutes of data collected on a quadruped robot. To accurately model the robot’s dynamics over a long horizon, we introduce a loss function that tracks the model’s prediction over multiple timesteps. We adapt model predictive control to account for planning latency, which allows the learned model to be used for real time control. Additionally, to ensure safe exploration during model learning, we embed prior knowledge of leg trajectories into the action space. The resulting system achieves fast and robust locomotion. Unlike model-free methods, which optimize for a particular task, our planner can use the same learned dynamics for various tasks, simply by changing the reward function.1 To the best of our knowledge, our approach is more than an order of magnitude more sample efficient than current model-free methods.</p></div></details></div><div><details><summary class="pl-0.5 cursor-pointer hover:text-primary hover:underline"><i class="fas fa-quote-right"></i> <span class="pl-0.5">Citation</span></summary><div class="px-4 py-2"><div class="relative"><div id="clipboard-47"><pre class="language-tex"><code class="language-tex">@InProceedings{puranic2020signal,
  title = {Learning from Demonstrations using Signal Temporal Logic},
  author = {Aniruddh Puranic and Jyotirmoy Deshmukh and Stefanos Nikolaidis},
  booktitle = {Conference on Robot Learning},
  year = {2020},
  month = {November},
  abstract = {We present a model-based reinforcement learning framework for robot locomotion that achieves walking based on only 4.5 minutes of data collected on a quadruped robot. To accurately model the robot’s dynamics over a long horizon, we introduce a loss function that tracks the model’s prediction over multiple timesteps. We adapt model predictive control to account for planning latency, which allows the learned model to be used for real time control. Additionally, to ensure safe exploration during model learning, we embed prior knowledge of leg trajectories into the action space. The resulting system achieves fast and robust locomotion. Unlike model-free methods, which optimize for a particular task, our planner can use the same learned dynamics for various tasks, simply by changing the reward function.1 To the best of our knowledge, our approach is more than an order of magnitude more sample efficient than current model-free methods.}
}
</code></pre></div><button class="cursor-pointer absolute right-0 top-0 clipboard flex focus:bg-gray-800 focus:bg-opacity-50 focus:outline-none focus:text-white group hover:bg-gray-800 hover:bg-opacity-50 hover:text-white items-center p-2 text-gray-400 text-xs transition" data-clipboard-target="#clipboard-47"><span class="hidden pr-2 group-focus:hidden group-hover:inline-block">Copy </span><span class="hidden pr-2 group-focus:inline-block group-hover:hidden">Copied! </span><span class="material-icons">content_copy</span></button></div></div></details></div></div></div><div class="w-full lg:w-1/2"><div class="w-full border-gray-100 border-t-2 hover:bg-gray-100 p-3"><a href="%20http://proceedings.mlr.press/v124/chen20a.html%20" class="text-black block font-bold hover:text-primary">Fair Contextual Multi-Armed Bandits: Theory and Experiments</a><p class="text-gray-600 text-base">Yifang Chen, Alex Cuellar, Haipeng Luo, Jignesh Modi, Heramb Nemlekar, Stefanos Nikolaidis</p><p class="text-gray-600 italic text-sm">36th Conference on Uncertainty in Artificial Intelligence (UAI), August 2020</p><div class="my-1"><a class="mr-3" href="https://arxiv.org/abs/1912.08055"><span class="pr-0.5 text-xl"><i class="ai ai-arxiv"></i></span> <span class="">arXiv</span> </a><a class="mr-3" href="http://proceedings.mlr.press/v124/chen20a.html"><span class="pr-0.5 text-lg"><i class="fas fa-book"></i></span> <span class="">PMLR</span></a></div><div><details><summary class="pl-0.5 cursor-pointer hover:text-primary hover:underline"><i class="far fa-file-alt"></i> <span class="pl-0.5">Abstract</span></summary><div class="px-4 py-2"><p>When an AI system interacts with multiple users, it frequently needs to make allocation decisions. For instance, a virtual agent decides whom to pay attention to in a group, or a factory robot selects a worker to deliver a part.Demonstrating fairness in decision making is essential for such systems to be broadly accepted. We introduce a Multi-Armed Bandit algorithm with fairness constraints, where fairness is defined as a minimum rate at which a task or a resource is assigned to a user. The proposed algorithm uses contextual information about the users and the task and makes no assumptions on how the losses capturing the performance of different users are generated. We provide theoretical guarantees of performance and empirical results from simulation and an online user study. The results highlight the benefit of accounting for contexts in fair decision making, especially when users perform better at some contexts and worse at others.</p></div></details></div><div><details><summary class="pl-0.5 cursor-pointer hover:text-primary hover:underline"><i class="fas fa-quote-right"></i> <span class="pl-0.5">Citation</span></summary><div class="px-4 py-2"><div class="relative"><div id="clipboard-48"><pre class="language-tex"><code class="language-tex">@InProceedings{chen2020experiments,
  title = {Fair Contextual Multi-Armed Bandits: Theory and Experiments},
  author = {Chen, Yifang and Cuellar, Alex and Luo, Haipeng and Modi, Jignesh and Nemlekar, Heramb and Nikolaidis, Stefanos},
  booktitle = {Proceedings of the 36th Conference on Uncertainty in Artificial Intelligence (UAI)},
  pages = {181--190},
  year = {2020},
  editor = {Jonas Peters and David Sontag},
  volume = {124},
  series = {Proceedings of Machine Learning Research},
  month = {03--06 Aug},
  publisher = {PMLR},
  pdf = {http://proceedings.mlr.press/v124/chen20a/chen20a.pdf},
  url = { http://proceedings.mlr.press/v124/chen20a.html },
  abstract = {When an AI system interacts with multiple users, it frequently needs to make allocation decisions. For instance, a virtual agent decides whom to pay attention to in a group, or a factory robot selects a worker to deliver a part.Demonstrating fairness in decision making is essential for such systems to be broadly accepted. We introduce a Multi-Armed Bandit algorithm with fairness constraints, where fairness is defined as a minimum rate at which a task or a resource is assigned to a user. The proposed algorithm uses contextual information about the users and the task and makes no assumptions on how the losses capturing the performance of different users are generated. We provide theoretical guarantees of performance and empirical results from simulation and an online user study. The results highlight the benefit of accounting for contexts in fair decision making, especially when users perform better at some contexts and worse at others.},
}
</code></pre></div><button class="cursor-pointer absolute right-0 top-0 clipboard flex focus:bg-gray-800 focus:bg-opacity-50 focus:outline-none focus:text-white group hover:bg-gray-800 hover:bg-opacity-50 hover:text-white items-center p-2 text-gray-400 text-xs transition" data-clipboard-target="#clipboard-48"><span class="hidden pr-2 group-focus:hidden group-hover:inline-block">Copy </span><span class="hidden pr-2 group-focus:inline-block group-hover:hidden">Copied! </span><span class="material-icons">content_copy</span></button></div></div></details></div></div></div><div class="w-full lg:w-1/2"><div class="w-full border-gray-100 border-t-2 hover:bg-gray-100 p-3"><a href="https://doi.org/10.1145/3319502.3374806" class="text-black block font-bold hover:text-primary">Multi-Armed Bandits with Fairness Constraints for Distributing Resources to Human Teammates</a><p class="text-gray-600 text-base">Houston Claure, Yifang Chen, Jignesh Modi, Malte Jung, Stefanos Nikolaidis</p><p class="text-gray-600 italic text-sm">2020 ACM/IEEE International Conference on Human-Robot Interaction, March 2020</p><div class="my-1"><a class="mr-3" href="https://dl.acm.org/doi/abs/10.1145/3319502.3374806"><span class="pr-0.5 text-lg"><i class="ai ai-acmdl"></i></span> <span class="">ACM Digital Library</span> </a><a class="mr-3" href="https://arxiv.org/abs/1907.00313"><span class="pr-0.5 text-xl"><i class="ai ai-arxiv"></i></span> <span class="">arXiv</span></a></div><div><details><summary class="pl-0.5 cursor-pointer hover:text-primary hover:underline"><i class="far fa-file-alt"></i> <span class="pl-0.5">Abstract</span></summary><div class="px-4 py-2"><p>How should a robot that collaborates with multiple people decide upon the distribution of resources (e.g. social attention, or parts needed for an assembly)? People are uniquely attuned to how resources are distributed. A decision to distribute more resources to one team member than another might be perceived as unfair with potentially detrimental effects for trust. We introduce a multi-armed bandit algorithm with fairness constraints, where a robot distributes resources to human teammates of different skill levels. In this problem, the robot does not know the skill level of each human teammate, but learns it by observing their performance over time. We define fairness as a constraint on the minimum rate that each human teammate is selected throughout the task. We provide theoretical guarantees on performance and perform a large-scale user study, where we adjust the level of fairness in our algorithm. Results show that fairness in resource distribution has a significant effect on users' trust in the system.</p></div></details></div><div><details><summary class="pl-0.5 cursor-pointer hover:text-primary hover:underline"><i class="fas fa-quote-right"></i> <span class="pl-0.5">Citation</span></summary><div class="px-4 py-2"><div class="relative"><div id="clipboard-49"><pre class="language-tex"><code class="language-tex">@inproceedings{claure2020bandits,
  author = {Claure, Houston and Chen, Yifang and Modi, Jignesh and Jung, Malte and Nikolaidis, Stefanos},
  title = {Multi-Armed Bandits with Fairness Constraints for Distributing Resources to Human Teammates},
  year = {2020},
  month = {March},
  isbn = {9781450367462},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3319502.3374806},
  doi = {10.1145/3319502.3374806},
  abstract = {How should a robot that collaborates with multiple people decide upon the distribution of resources (e.g. social attention, or parts needed for an assembly)? People are uniquely attuned to how resources are distributed. A decision to distribute more resources to one team member than another might be perceived as unfair with potentially detrimental effects for trust. We introduce a multi-armed bandit algorithm with fairness constraints, where a robot distributes resources to human teammates of different skill levels. In this problem, the robot does not know the skill level of each human teammate, but learns it by observing their performance over time. We define fairness as a constraint on the minimum rate that each human teammate is selected throughout the task. We provide theoretical guarantees on performance and perform a large-scale user study, where we adjust the level of fairness in our algorithm. Results show that fairness in resource distribution has a significant effect on users' trust in the system.},
  booktitle = {Proceedings of the 2020 ACM/IEEE International Conference on Human-Robot Interaction},
  pages = {299–308},
  numpages = {10},
  keywords = {reinforcement learning, multi-armed bandits, trust, fairness},
  location = {Cambridge, United Kingdom},
  series = {HRI '20}
}
</code></pre></div><button class="cursor-pointer absolute right-0 top-0 clipboard flex focus:bg-gray-800 focus:bg-opacity-50 focus:outline-none focus:text-white group hover:bg-gray-800 hover:bg-opacity-50 hover:text-white items-center p-2 text-gray-400 text-xs transition" data-clipboard-target="#clipboard-49"><span class="hidden pr-2 group-focus:hidden group-hover:inline-block">Copy </span><span class="hidden pr-2 group-focus:inline-block group-hover:hidden">Copied! </span><span class="material-icons">content_copy</span></button></div></div></details></div></div></div><div class="w-full lg:w-1/2"><div class="w-full border-gray-100 border-t-2 hover:bg-gray-100 p-3"><a href="http://www.stefanosnikolaidis.net/papers/zhang_isrr_2019.pdf" class="text-black block font-bold hover:text-primary">Learning Collaborative Action Plans from Unlabeled YouTube Videos</a><p class="text-gray-600 text-base">Hejia Zhang, Po-Jen Lai, Sayan Paul, Suraj Kothawade, Stefanos Nikolaidis</p><p class="text-gray-600 italic text-sm">Robotics Research, The 19th International Symposium, ISRR 2019, October 2019</p><div class="my-1"><a class="mr-3" href="http://www.stefanosnikolaidis.net/papers/zhang_isrr_2019.pdf"><span class="pr-0.5 text-lg"><i class="far fa-file-pdf"></i></span> <span class="">PDF</span></a></div><div><details><summary class="pl-0.5 cursor-pointer hover:text-primary hover:underline"><i class="far fa-file-alt"></i> <span class="pl-0.5">Abstract</span></summary><div class="px-4 py-2"><p>Videos from the World Wide Web provide a rich source of information that robots could use to acquire knowledge about manipulation tasks. Previous work has focused on generating action sequences from unconstrained videos for a single robot performing manipulation tasks by itself. However, robots operating in the same physical space with people need to not only perform actions autonomously, but also coordinate seamlessly with their human counterparts. This often requires representing and executing collaborative manipulation actions, such as handing over a tool or holding an object for the other agent. We present a system for knowledge acquisition of collaborative manipulation action plans that outputs commands to the robot in the form of visual sentence. We show the performance of the system in 12 unlabeled action clips taken from collaborative cooking videos on YouTube. We view this as the first step towards extracting collaborative manipulation action sequences from unconstrained, unlabeled online videos.</p></div></details></div><div><details><summary class="pl-0.5 cursor-pointer hover:text-primary hover:underline"><i class="fas fa-quote-right"></i> <span class="pl-0.5">Citation</span></summary><div class="px-4 py-2"><div class="relative"><div id="clipboard-50"><pre class="language-tex"><code class="language-tex">@inproceedings {zhang2019youtube,
  author={Hejia Zhang and Po-Jen Lai and Sayan Paul and Suraj Kothawade and Stefanos Nikolaidis},
  title={Learning Collaborative Action Plans from Unlabeled YouTube Videos},
  booktitle={Robotics Research, The 19th International Symposium, {ISRR} 2019},
  location={Hanoi, Vietnam},
  year={2019},
  month={October},
  abstract={Videos from the World Wide Web provide a rich source of information that robots could use to acquire knowledge about manipulation tasks. Previous work has focused on generating action sequences from unconstrained videos for a single robot performing manipulation tasks by itself. However, robots operating in the same physical space with people need to not only perform actions autonomously, but also coordinate seamlessly with their human counterparts. This often requires representing and executing collaborative manipulation actions, such as handing over a tool or holding an object for the other agent. We present a system for knowledge acquisition of collaborative manipulation action plans that outputs commands to the robot in the form of visual sentence. We show the performance of the system in 12 unlabeled action clips taken from collaborative cooking videos on YouTube. We view this as the first step towards extracting collaborative manipulation action sequences from unconstrained, unlabeled online videos.},
}
</code></pre></div><button class="cursor-pointer absolute right-0 top-0 clipboard flex focus:bg-gray-800 focus:bg-opacity-50 focus:outline-none focus:text-white group hover:bg-gray-800 hover:bg-opacity-50 hover:text-white items-center p-2 text-gray-400 text-xs transition" data-clipboard-target="#clipboard-50"><span class="hidden pr-2 group-focus:hidden group-hover:inline-block">Copy </span><span class="hidden pr-2 group-focus:inline-block group-hover:hidden">Copied! </span><span class="material-icons">content_copy</span></button></div></div></details></div></div></div></div></div><div class="max-w-none prose mb-4 mt-8"><h2 id="research-contribution-%232%3A-automatic-scenario-generation">Research Contribution #2: Automatic Scenario Generation <a class="permalink" href="#research-contribution-%232%3A-automatic-scenario-generation">¶</a></h2></div><div class="flex items-center flex-wrap"><div class="w-full lg:w-7/12 max-w-none prose"><p>For HRI systems to be widely accepted and used, they need to perform robustly in human environments. Traditionally, human-robot interaction (HRI) algorithms are tested with human subject experiments. While these experiments are fundamental in exploring and evaluating human-robot interactions and can lead to exciting and unpredictable behaviors, they are often limited in the number of environments and human actions that they can cover. Exhaustive search of human actions and environments is also computationally prohibitive. This highlights a critical need for automatically generating HRI scenarios; failure to do so can lead to infrequent scenarios that are undiscovered during testing but occur in large-scale real-world deployments, resulting in potentially costly failures.</p><p>Our goal is to find scenarios that are diverse, complex and realistic. Our insight is to formulate this as a quality diversity problem, where the goal is to generate scenarios that are diverse with respect to specific measures of interest. Drawing upon insights from procedural content generation, we integrate state-of-the-art quality diversity algorithms with generative models trained with human examples to generate diverse scenarios in simulation that are also complex and realistic.</p></div><div class="w-full lg:pl-4 lg:w-5/12 py-4"><div class="w-full relative h-0" style="padding-bottom:56.25%"><iframe title="RSS 2021, Spotlight Talk 56: On the Importance of Environments in Human-Robot Coordination" class="w-full absolute h-full left-0 top-0" src="https://www.youtube.com/embed/IeoTM_KnDfY" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div></div></div><div class="mt-4"><div class="flex flex-wrap"><div class="w-full lg:w-1/2"><div class="w-full border-gray-100 border-t-2 hover:bg-gray-100 p-3"><a href="" class="text-black block font-bold hover:text-primary">Evaluating human-robot interaction algorithms in shared autonomy via quality diversity scenario generation</a><p class="text-gray-600 text-base">Matthew C. Fontaine, Stefanos Nikolaidis</p><p class="text-gray-600 italic text-sm">ACM Transactions on Human-Robot Interaction (THRI) (in press), 2021</p><div class="my-1"></div><div><details><summary class="pl-0.5 cursor-pointer hover:text-primary hover:underline"><i class="far fa-file-alt"></i> <span class="pl-0.5">Abstract</span></summary><div class="px-4 py-2"><p></p></div></details></div><div><details><summary class="pl-0.5 cursor-pointer hover:text-primary hover:underline"><i class="fas fa-quote-right"></i> <span class="pl-0.5">Citation</span></summary><div class="px-4 py-2"><div class="relative"><div id="clipboard-51"><pre class="language-tex"><code class="language-tex">@article{fontaine2021evaluating,
    author={Matthew C. Fontaine and Stefanos Nikolaidis},
    year={2021},
    title={Evaluating human-robot interaction algorithms in shared autonomy via quality diversity scenario generation},
    journal={ACM Transactions on Human-Robot Interaction (THRI) (in press)},
}
</code></pre></div><button class="cursor-pointer absolute right-0 top-0 clipboard flex focus:bg-gray-800 focus:bg-opacity-50 focus:outline-none focus:text-white group hover:bg-gray-800 hover:bg-opacity-50 hover:text-white items-center p-2 text-gray-400 text-xs transition" data-clipboard-target="#clipboard-51"><span class="hidden pr-2 group-focus:hidden group-hover:inline-block">Copy </span><span class="hidden pr-2 group-focus:inline-block group-hover:hidden">Copied! </span><span class="material-icons">content_copy</span></button></div></div></details></div></div></div><div class="w-full lg:w-1/2"><div class="w-full border-gray-100 border-t-2 hover:bg-gray-100 p-3"><a href="https://arxiv.org/abs/2106.10853" class="text-black block font-bold hover:text-primary">On the Importance of Environments in Human-Robot Coordination</a><p class="text-gray-600 text-base">Matthew C. Fontaine*, Ya-Chuan Hsu*, Yulun Zhang*, Bryon Tjanaka, Stefanos Nikolaidis</p><p class="text-gray-600 italic text-sm">Robotics: Science and Systems, July 2021</p><div class="my-1"><a class="mr-3" href="https://arxiv.org/abs/2106.10853"><span class="pr-0.5 text-xl"><i class="ai ai-arxiv"></i></span> <span class="">arXiv</span> </a><a class="mr-3" href="https://overcooked-lsi.github.io/"><span class="pr-0.5 text-lg"><i class="fas fa-globe-americas"></i></span> <span class="">Website</span></a></div><div><details><summary class="pl-0.5 cursor-pointer hover:text-primary hover:underline"><i class="far fa-file-alt"></i> <span class="pl-0.5">Abstract</span></summary><div class="px-4 py-2"><p>When studying robots collaborating with humans, much of the focus has been on robot policies that coordinate fluently with human teammates in collaborative tasks. However, less emphasis has been placed on the effect of the environment on coordination behaviors. To thoroughly explore environments that result in diverse behaviors, we propose a framework for procedural generation of environments that are (1) stylistically similar to human-authored environments, (2) guaranteed to be solvable by the human-robot team, and (3) diverse with respect to coordination measures. We analyze the procedurally generated environments in the Overcooked benchmark domain via simulation and an online user study. Results show that the environments result in qualitatively different emerging behaviors and statistically significant differences in collaborative fluency metrics, even when the robot runs the same planning algorithm.</p></div></details></div><div><details><summary class="pl-0.5 cursor-pointer hover:text-primary hover:underline"><i class="fas fa-quote-right"></i> <span class="pl-0.5">Citation</span></summary><div class="px-4 py-2"><div class="relative"><div id="clipboard-52"><pre class="language-tex"><code class="language-tex">@inproceedings{fontaine2021importance,
    title={On the Importance of Environments in Human-Robot Coordination},
    author={Matthew C. Fontaine and Ya-Chuan Hsu and Yulun Zhang and Bryon Tjanaka and Stefanos Nikolaidis},
    year={2021},
    month={July},
    url={https://arxiv.org/abs/2106.10853},
    booktitle={Proceedings of Robotics: Science and Systems},
    doi={10.15607/RSS.2021.XVII.038},
    abstract={When studying robots collaborating with humans, much of the focus has been on robot policies that coordinate fluently with human teammates in collaborative tasks. However, less emphasis has been placed on the effect of the environment on coordination behaviors. To thoroughly explore environments that result in diverse behaviors, we propose a framework for procedural generation of environments that are (1) stylistically similar to human-authored environments, (2) guaranteed to be solvable by the human-robot team, and (3) diverse with respect to coordination measures. We analyze the procedurally generated environments in the Overcooked benchmark domain via simulation and an online user study. Results show that the environments result in qualitatively different emerging behaviors and statistically significant differences in collaborative fluency metrics, even when the robot runs the same planning algorithm.}
}
</code></pre></div><button class="cursor-pointer absolute right-0 top-0 clipboard flex focus:bg-gray-800 focus:bg-opacity-50 focus:outline-none focus:text-white group hover:bg-gray-800 hover:bg-opacity-50 hover:text-white items-center p-2 text-gray-400 text-xs transition" data-clipboard-target="#clipboard-52"><span class="hidden pr-2 group-focus:hidden group-hover:inline-block">Copy </span><span class="hidden pr-2 group-focus:inline-block group-hover:hidden">Copied! </span><span class="material-icons">content_copy</span></button></div></div></details></div></div></div><div class="w-full lg:w-1/2"><div class="w-full border-gray-100 border-t-2 hover:bg-gray-100 p-3"><a href="https://arxiv.org/abs/2012.04283" class="text-black block font-bold hover:text-primary">A Quality Diversity Approach to Automatically Generating Human-Robot Interaction Scenarios in Shared Autonomy</a><p class="text-gray-600 text-base">Matthew C. Fontaine, Stefanos Nikolaidis</p><p class="text-gray-600 italic text-sm">Robotics: Science and Systems, July 2021</p><div class="my-1"><a class="mr-3" href="https://arxiv.org/abs/2012.04283"><span class="pr-0.5 text-xl"><i class="ai ai-arxiv"></i></span> <span class="">arXiv</span></a></div><div><details><summary class="pl-0.5 cursor-pointer hover:text-primary hover:underline"><i class="far fa-file-alt"></i> <span class="pl-0.5">Abstract</span></summary><div class="px-4 py-2"><p>The growth of scale and complexity of interactions between humans and robots highlights the need for new computational methods to automatically evaluate novel algorithms and applications. Exploring diverse scenarios of humans and robots interacting in simulation can improve understanding of the robotic system and avoid potentially costly failures in real-world settings. We formulate this problem as a quality diversity (QD) problem, where the goal is to discover diverse failure scenarios by simultaneously exploring both environments and human actions. We focus on the shared autonomy domain, where the robot attempts to infer the goal of a human operator, and adopt the QD algorithm MAP-Elites to generate scenarios for two published algorithms in this domain: shared autonomy via hindsight optimization and linear policy blending. Some of the generated scenarios confirm previous theoretical findings, while others are surprising and bring about a new understanding of state-of-the-art implementations. Our experiments show that MAP-Elites outperforms Monte-Carlo simulation and optimization based methods in effectively searching the scenario space, highlighting its promise for automatic evaluation of algorithms in human-robot interaction.</p></div></details></div><div><details><summary class="pl-0.5 cursor-pointer hover:text-primary hover:underline"><i class="fas fa-quote-right"></i> <span class="pl-0.5">Citation</span></summary><div class="px-4 py-2"><div class="relative"><div id="clipboard-53"><pre class="language-tex"><code class="language-tex">@inproceedings{fontaine2021shared,
    title={A Quality Diversity Approach to Automatically Generating Human-Robot
           Interaction Scenarios in Shared Autonomy},
    author={Matthew C. Fontaine and Stefanos Nikolaidis},
    year={2021},
    month={July},
    url={https://arxiv.org/abs/2012.04283},
    booktitle={Proceedings of Robotics: Science and Systems},
    doi={10.15607/RSS.2021.XVII.036},
    abstract={The growth of scale and complexity of interactions between humans and robots highlights the need for new computational methods to automatically evaluate novel algorithms and applications. Exploring diverse scenarios of humans and robots interacting in simulation can improve understanding of the robotic system and avoid potentially costly failures in real-world settings. We formulate this problem as a quality diversity (QD) problem, where the goal is to discover diverse failure scenarios by simultaneously exploring both environments and human actions. We focus on the shared autonomy domain, where the robot attempts to infer the goal of a human operator, and adopt the QD algorithm MAP-Elites to generate scenarios for two published algorithms in this domain: shared autonomy via hindsight optimization and linear policy blending. Some of the generated scenarios confirm previous theoretical findings, while others are surprising and bring about a new understanding of state-of-the-art implementations. Our experiments show that MAP-Elites outperforms Monte-Carlo simulation and optimization based methods in effectively searching the scenario space, highlighting its promise for automatic evaluation of algorithms in human-robot interaction.}
}
</code></pre></div><button class="cursor-pointer absolute right-0 top-0 clipboard flex focus:bg-gray-800 focus:bg-opacity-50 focus:outline-none focus:text-white group hover:bg-gray-800 hover:bg-opacity-50 hover:text-white items-center p-2 text-gray-400 text-xs transition" data-clipboard-target="#clipboard-53"><span class="hidden pr-2 group-focus:hidden group-hover:inline-block">Copy </span><span class="hidden pr-2 group-focus:inline-block group-hover:hidden">Copied! </span><span class="material-icons">content_copy</span></button></div></div></details></div></div></div><div class="w-full lg:w-1/2"><div class="w-full border-gray-100 border-t-2 hover:bg-gray-100 p-3"><a href="https://ojs.aaai.org/index.php/AAAI/article/view/16740" class="text-black block font-bold hover:text-primary">Illuminating Mario Scenes in the Latent Space of a Generative Adversarial Network</a><p class="text-gray-600 text-base">Matthew C. Fontaine, Ruilin Liu, Ahmed Khalifa, Jignesh Modi, Julian Togelius, Amy K. Hoover, Stefanos Nikolaidis</p><p class="text-gray-600 italic text-sm">AAAI Conference on Artificial Intelligence, February 2021</p><div class="my-1"><a class="mr-3" href="https://ojs.aaai.org/index.php/AAAI/article/view/16740"><span class="pr-0.5 text-lg"><i class="fas fa-book"></i></span> <span class="">AAAI</span> </a><a class="mr-3" href="https://arxiv.org/abs/2007.05674"><span class="pr-0.5 text-xl"><i class="ai ai-arxiv"></i></span> <span class="">arXiv</span> </a><a class="mr-3" href="https://github.com/icaros-usc/MarioGAN-LSI"><span class="pr-0.5 text-lg"><i class="fab fa-github"></i></span> <span class="">GitHub</span></a></div><div><details><summary class="pl-0.5 cursor-pointer hover:text-primary hover:underline"><i class="far fa-file-alt"></i> <span class="pl-0.5">Abstract</span></summary><div class="px-4 py-2"><p>Generative adversarial networks (GANs) are quickly becoming a ubiquitous approach to procedurally generating video game levels. While GAN generated levels are stylistically similar to human-authored examples, human designers often want to explore the generative design space of GANs to extract interesting levels. However, human designers find latent vectors opaque and would rather explore along dimensions the designer specifies, such as number of enemies or obstacles. We propose using state-of-the-art quality diversity algorithms designed to optimize continuous spaces, i.e. MAP-Elites with a directional variation operator and Covariance Matrix Adaptation MAP-Elites, to efficiently explore the latent space of a GAN to extract levels that vary across a set of specified gameplay measures. In the benchmark domain of Super Mario Bros, we demonstrate how designers may specify gameplay measures to our system and extract high-quality (playable) levels with a diverse range of level mechanics, while still maintaining stylistic similarity to human authored examples. An online user study shows how the different mechanics of the automatically generated levels affect subjective ratings of their perceived difficulty and appearance.</p></div></details></div><div><details><summary class="pl-0.5 cursor-pointer hover:text-primary hover:underline"><i class="fas fa-quote-right"></i> <span class="pl-0.5">Citation</span></summary><div class="px-4 py-2"><div class="relative"><div id="clipboard-54"><pre class="language-tex"><code class="language-tex">@article{fontaine2021illuminating,
  title={Illuminating Mario Scenes in the Latent Space of a Generative Adversarial Network},
  volume={35},
  url={https://ojs.aaai.org/index.php/AAAI/article/view/16740},
  journal={AAAI Conference on Artificial Intelligence},
  author={Matthew C. Fontaine and Ruilin Liu and Ahmed Khalifa and Jignesh Modi and Julian Togelius and Amy K. Hoover and Stefanos Nikolaidis},
  year={2021},
  month={February},
  abstract={Generative adversarial networks (GANs) are quickly becoming a ubiquitous approach to procedurally generating video game levels. While GAN generated levels are stylistically similar to human-authored examples, human designers often want to explore the generative design space of GANs to extract interesting levels. However, human designers find latent vectors opaque and would rather explore along dimensions the designer specifies, such as number of enemies or obstacles. We propose using state-of-the-art quality diversity algorithms designed to optimize continuous spaces, i.e. MAP-Elites with a directional variation operator and Covariance Matrix Adaptation MAP-Elites, to efficiently explore the latent space of a GAN to extract levels that vary across a set of specified gameplay measures. In the benchmark domain of Super Mario Bros, we demonstrate how designers may specify gameplay measures to our system and extract high-quality (playable) levels with a diverse range of level mechanics, while still maintaining stylistic similarity to human authored examples. An online user study shows how the different mechanics of the automatically generated levels affect subjective ratings of their perceived difficulty and appearance.}
}
</code></pre></div><button class="cursor-pointer absolute right-0 top-0 clipboard flex focus:bg-gray-800 focus:bg-opacity-50 focus:outline-none focus:text-white group hover:bg-gray-800 hover:bg-opacity-50 hover:text-white items-center p-2 text-gray-400 text-xs transition" data-clipboard-target="#clipboard-54"><span class="hidden pr-2 group-focus:hidden group-hover:inline-block">Copy </span><span class="hidden pr-2 group-focus:inline-block group-hover:hidden">Copied! </span><span class="material-icons">content_copy</span></button></div></div></details></div></div></div><div class="w-full lg:w-1/2"><div class="w-full border-gray-100 border-t-2 hover:bg-gray-100 p-3"><a href="https://ojs.aaai.org/index.php/AIIDE/article/view/7424" class="text-black block font-bold hover:text-primary">Video Game Level Repair via Mixed Integer Linear Programming</a><p class="text-gray-600 text-base">Hejia Zhang*, Matthew Fontaine*, Amy Hoover, Julian Togelius, Bistra Dilkina, Stefanos Nikolaidis</p><p class="text-gray-600 italic text-sm">AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, October 2020</p><div class="my-1"><a class="mr-3" href="https://ojs.aaai.org/index.php/AIIDE/article/view/7424"><span class="pr-0.5 text-lg"><i class="fas fa-book"></i></span> <span class="">AAAI</span></a></div><div><details><summary class="pl-0.5 cursor-pointer hover:text-primary hover:underline"><i class="far fa-file-alt"></i> <span class="pl-0.5">Abstract</span></summary><div class="px-4 py-2"><p>Recent advancements in procedural content generation via machine learning enable the generation of video-game levels that are aesthetically similar to human-authored examples. However, the generated levels are often unplayable without additional editing. We propose a “generate-then-repair” framework for automatic generation of playable levels adhering to specific styles. The framework constructs levels using a generative adversarial network (GAN) trained with human-authored examples and repairs them using a mixed-integer linear program (MIP) with playability constraints. A key component of the framework is computing minimum cost edits between the GAN generated level and the solution of the MIP solver, which we cast as a minimum cost network flow problem. Results show that the proposed framework generates a diverse range of playable levels, that capture the spatial relationships between objects exhibited in the human-authored levels.</p></div></details></div><div><details><summary class="pl-0.5 cursor-pointer hover:text-primary hover:underline"><i class="fas fa-quote-right"></i> <span class="pl-0.5">Citation</span></summary><div class="px-4 py-2"><div class="relative"><div id="clipboard-55"><pre class="language-tex"><code class="language-tex">@article{zhang2020repair,
  title={Video Game Level Repair via Mixed Integer Linear Programming},
  volume={16},
  url={https://ojs.aaai.org/index.php/AIIDE/article/view/7424},
  abstract={Recent advancements in procedural content generation via machine learning enable the generation of video-game levels that are aesthetically similar to human-authored examples. However, the generated levels are often unplayable without additional editing. We propose a “generate-then-repair” framework for automatic generation of playable levels adhering to specific styles. The framework constructs levels using a generative adversarial network (GAN) trained with human-authored examples and repairs them using a mixed-integer linear program (MIP) with playability constraints. A key component of the framework is computing minimum cost edits between the GAN generated level and the solution of the MIP solver, which we cast as a minimum cost network flow problem. Results show that the proposed framework generates a diverse range of playable levels, that capture the spatial relationships between objects exhibited in the human-authored levels.},
  number={1},
  journal={Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment},
  author={Zhang, Hejia and Fontaine, Matthew and Hoover, Amy and Togelius, Julian and Dilkina, Bistra and Nikolaidis, Stefanos},
  year={2020},
  month={October},
  pages={151-158}
}
</code></pre></div><button class="cursor-pointer absolute right-0 top-0 clipboard flex focus:bg-gray-800 focus:bg-opacity-50 focus:outline-none focus:text-white group hover:bg-gray-800 hover:bg-opacity-50 hover:text-white items-center p-2 text-gray-400 text-xs transition" data-clipboard-target="#clipboard-55"><span class="hidden pr-2 group-focus:hidden group-hover:inline-block">Copy </span><span class="hidden pr-2 group-focus:inline-block group-hover:hidden">Copied! </span><span class="material-icons">content_copy</span></button></div></div></details></div></div></div></div></div><div class="max-w-none prose mb-4 mt-8"><h2 id="research-contribution-%233%3A-quality-diversity-optimization">Research Contribution #3: Quality Diversity Optimization <a class="permalink" href="#research-contribution-%233%3A-quality-diversity-optimization">¶</a></h2></div><div class="flex items-center flex-wrap"><div class="w-full lg:w-7/12 max-w-none prose"><p>Searching the vast space of HRI scenarios requires algorithms that explore efficiently very high-dimensional continuous domains. We focus on a class of stochastic optimization algorithms, named quality diversity (QD) algorithms, which search for a range of high-quality solutions that are diverse with respect to measures of interest. Current state-of-the-art QD algorithms find new solutions by perturbing existing high-quality solutions with Gaussian noise, or through variation operators that exploit local correlations. However, these algorithms face difficulties when the low-dimensional measure space that we wish to cover is distorted, e.g., when uniformly sampling the space of scenarios results in scenarios that are nearly identical with respect to the measures that we wish to have diversity for.</p><p>Our insight is that we can leverage the adaptation properties of the CMA-ES derivative-free optimization algorithm to dynamically adapt the step size of our search based on how our archive of solutions is changing. Using this insight we propose a new class of algorithms that bring together the self-adaptation of CMA-ES with the archiving properties of MAP-Elites to approximate a natural gradient of the quality diversity objective. Our algorithms can also leverage the gradient information of the objective and measure functions to achieve significant gains in performance.</p></div><div class="w-full lg:pl-4 lg:w-5/12"><img class="w-full" alt="Quality diversity algorithms at work." src="/imgs/lin_proj_combo.gif" loading="lazy"></div></div><div class="mt-4"><div class="flex flex-wrap"><div class="w-full lg:w-1/2"><div class="w-full border-gray-100 border-t-2 hover:bg-gray-100 p-3"><a href="https://arxiv.org/abs/2106.03894" class="text-black block font-bold hover:text-primary">Differentiable Quality Diversity</a><p class="text-gray-600 text-base">Matthew C. Fontaine, Stefanos Nikolaidis</p><p class="text-gray-600 italic text-sm">Advances in Neural Information Processing Systems, June 2021</p><p>NeurIPS 2021 Oral</p><div class="my-1"><a class="mr-3" href="https://arxiv.org/abs/2106.03894"><span class="pr-0.5 text-xl"><i class="ai ai-arxiv"></i></span> <span class="">arXiv</span> </a><a class="mr-3" href="https://github.com/icaros-usc/dqd"><span class="pr-0.5 text-lg"><i class="fab fa-github"></i></span> <span class="">GitHub</span></a></div><div><details><summary class="pl-0.5 cursor-pointer hover:text-primary hover:underline"><i class="far fa-file-alt"></i> <span class="pl-0.5">Abstract</span></summary><div class="px-4 py-2"><p>Quality diversity (QD) is a growing branch of stochastic optimization research that studies the problem of generating an archive of solutions that maximize a given objective function but are also diverse with respect to a set of specified measure functions. However, even when these functions are differentiable, QD algorithms treat them as "black boxes", ignoring gradient information. We present the differentiable quality diversity (DQD) problem, a special case of QD, where both the objective and measure functions are first order differentiable. We then present MAP-Elites via Gradient Arborescence (MEGA), a DQD algorithm that leverages gradient information to efficiently explore the joint range of the objective and measure functions. Results in two QD benchmark domains and in searching the latent space of a StyleGAN show that MEGA significantly outperforms state-of-the-art QD algorithms, highlighting DQD's promise for efficient quality diversity optimization when gradient information is available.</p></div></details></div><div><details><summary class="pl-0.5 cursor-pointer hover:text-primary hover:underline"><i class="fas fa-quote-right"></i> <span class="pl-0.5">Citation</span></summary><div class="px-4 py-2"><div class="relative"><div id="clipboard-56"><pre class="language-tex"><code class="language-tex">@article{fontaine2021differentiable,
  title={Differentiable Quality Diversity},
  author={Matthew C. Fontaine and Stefanos Nikolaidis},
  booktitle={Advances in Neural Information Processing Systems},
  year={2021},
  month={June},
  url={https://arxiv.org/abs/2106.03894},
  journal={CoRR},
  abstract={Quality diversity (QD) is a growing branch of stochastic optimization research that studies the problem of generating an archive of solutions that maximize a given objective function but are also diverse with respect to a set of specified measure functions. However, even when these functions are differentiable, QD algorithms treat them as "black boxes", ignoring gradient information. We present the differentiable quality diversity (DQD) problem, a special case of QD, where both the objective and measure functions are first order differentiable. We then present MAP-Elites via Gradient Arborescence (MEGA), a DQD algorithm that leverages gradient information to efficiently explore the joint range of the objective and measure functions. Results in two QD benchmark domains and in searching the latent space of a StyleGAN show that MEGA significantly outperforms state-of-the-art QD algorithms, highlighting DQD's promise for efficient quality diversity optimization when gradient information is available.},
}
</code></pre></div><button class="cursor-pointer absolute right-0 top-0 clipboard flex focus:bg-gray-800 focus:bg-opacity-50 focus:outline-none focus:text-white group hover:bg-gray-800 hover:bg-opacity-50 hover:text-white items-center p-2 text-gray-400 text-xs transition" data-clipboard-target="#clipboard-56"><span class="hidden pr-2 group-focus:hidden group-hover:inline-block">Copy </span><span class="hidden pr-2 group-focus:inline-block group-hover:hidden">Copied! </span><span class="material-icons">content_copy</span></button></div></div></details></div></div></div><div class="w-full lg:w-1/2"><div class="w-full border-gray-100 border-t-2 hover:bg-gray-100 p-3"><a href="https://doi.org/10.1145/3377930.3390232" class="text-black block font-bold hover:text-primary">Covariance Matrix Adaptation for the Rapid Illumination of Behavior Space</a><p class="text-gray-600 text-base">Matthew C. Fontaine, Julian Togelius, Stefanos Nikolaidis, Amy K. Hoover</p><p class="text-gray-600 italic text-sm">2020 Genetic and Evolutionary Computation Conference, June 2020</p><div class="my-1"><a class="mr-3" href="https://arxiv.org/abs/1912.02400"><span class="pr-0.5 text-xl"><i class="ai ai-arxiv"></i></span> <span class="">arXiv</span> </a><a class="mr-3" href="https://dl.acm.org/doi/abs/10.1145/3377930.3390232"><span class="pr-0.5 text-lg"><i class="ai ai-acmdl"></i></span> <span class="">ACM Digital Library</span></a></div><div><details><summary class="pl-0.5 cursor-pointer hover:text-primary hover:underline"><i class="far fa-file-alt"></i> <span class="pl-0.5">Abstract</span></summary><div class="px-4 py-2"><p>We focus on the challenge of finding a diverse collection of quality solutions on complex continuous domains. While quality diversity (QD) algorithms like Novelty Search with Local Competition (NSLC) and MAP-Elites are designed to generate a diverse range of solutions, these algorithms require a large number of evaluations for exploration of continuous spaces. Meanwhile, variants of the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) are among the best-performing derivative-free optimizers in single-objective continuous domains. This paper proposes a new QD algorithm called Covariance Matrix Adaptation MAP-Elites (CMA-ME). Our new algorithm combines the self-adaptation techniques of CMA-ES with archiving and mapping techniques for maintaining diversity in QD. Results from experiments based on standard continuous optimization benchmarks show that CMA-ME finds better-quality solutions than MAP-Elites; similarly, results on the strategic game Hearthstone show that CMA-ME finds both a higher overall quality and broader diversity of strategies than both CMA-ES and MAP-Elites. Overall, CMA-ME more than doubles the performance of MAP-Elites using standard QD performance metrics. These results suggest that QD algorithms augmented by operators from state-of-the-art optimization algorithms can yield high-performing methods for simultaneously exploring and optimizing continuous search spaces, with significant applications to design, testing, and reinforcement learning among other domains.</p></div></details></div><div><details><summary class="pl-0.5 cursor-pointer hover:text-primary hover:underline"><i class="fas fa-quote-right"></i> <span class="pl-0.5">Citation</span></summary><div class="px-4 py-2"><div class="relative"><div id="clipboard-57"><pre class="language-tex"><code class="language-tex">@inproceedings{fontaine2020covariance,
  author = {Fontaine, Matthew C. and Togelius, Julian and Nikolaidis, Stefanos and Hoover, Amy K.},
  title = {Covariance Matrix Adaptation for the Rapid Illumination of Behavior Space},
  year = {2020},
  month = {June},
  isbn = {9781450371285},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3377930.3390232},
  doi = {10.1145/3377930.3390232},
  abstract = {We focus on the challenge of finding a diverse collection of quality solutions on complex continuous domains. While quality diversity (QD) algorithms like Novelty Search with Local Competition (NSLC) and MAP-Elites are designed to generate a diverse range of solutions, these algorithms require a large number of evaluations for exploration of continuous spaces. Meanwhile, variants of the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) are among the best-performing derivative-free optimizers in single-objective continuous domains. This paper proposes a new QD algorithm called Covariance Matrix Adaptation MAP-Elites (CMA-ME). Our new algorithm combines the self-adaptation techniques of CMA-ES with archiving and mapping techniques for maintaining diversity in QD. Results from experiments based on standard continuous optimization benchmarks show that CMA-ME finds better-quality solutions than MAP-Elites; similarly, results on the strategic game Hearthstone show that CMA-ME finds both a higher overall quality and broader diversity of strategies than both CMA-ES and MAP-Elites. Overall, CMA-ME more than doubles the performance of MAP-Elites using standard QD performance metrics. These results suggest that QD algorithms augmented by operators from state-of-the-art optimization algorithms can yield high-performing methods for simultaneously exploring and optimizing continuous search spaces, with significant applications to design, testing, and reinforcement learning among other domains.},
  booktitle = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference},
  pages = {94–102},
  numpages = {9},
  keywords = {hearthstone, evolutionary algorithms, MAP-Elites, optimization, quality diversity, illumination algorithms},
  location = {Canc'{u}n, Mexico},
  series = {GECCO '20}
}
</code></pre></div><button class="cursor-pointer absolute right-0 top-0 clipboard flex focus:bg-gray-800 focus:bg-opacity-50 focus:outline-none focus:text-white group hover:bg-gray-800 hover:bg-opacity-50 hover:text-white items-center p-2 text-gray-400 text-xs transition" data-clipboard-target="#clipboard-57"><span class="hidden pr-2 group-focus:hidden group-hover:inline-block">Copy </span><span class="hidden pr-2 group-focus:inline-block group-hover:hidden">Copied! </span><span class="material-icons">content_copy</span></button></div></div></details></div></div></div></div></div></div></main></div><div class="flex-grow"></div><footer><div class="w-full text-center bg-gray-100 p-6"><div class="mx-auto max-w-screen-2xl"><div class="flex flex-wrap justify-center mx-auto"><a href="https://www.youtube.com/channel/UCUuIwdCu3yXZBv-KtPS5TvQ" title="YouTube" class="flex items-center bg-white h-12 hover:bg-gray-200 hover:no-underline justify-center mx-1 rounded-full shadow-md text-2xl w-12" style="color:#f51d03"><i class="mx-auto fab fa-youtube"></i> </a><a href="https://twitter.com/icaroslab" title="Twitter (@icaroslab)" class="flex items-center bg-white h-12 hover:bg-gray-200 hover:no-underline justify-center mx-1 rounded-full shadow-md text-2xl w-12" style="color:#2f9bf0"><i class="mx-auto fab fa-twitter"></i> </a><a href="https://github.com/icaros-usc/" title="GitHub (icaros-usc)" class="flex items-center bg-white h-12 hover:bg-gray-200 hover:no-underline justify-center mx-1 rounded-full shadow-md text-2xl w-12" style="color:#000"><i class="mx-auto fab fa-github"></i> </a><a href="mailto:nikolaid@usc.edu" title="Email (nikolaid@usc.edu)" class="flex items-center bg-white h-12 hover:bg-gray-200 hover:no-underline justify-center mx-1 rounded-full shadow-md text-2xl w-12" style="color:#5f5f5f"><i class="fas fa-envelope mx-auto"></i></a></div><div class="w-full flex flex-wrap justify-center md:flex-nowrap pt-3 text-black"><p class="w-full md:w-auto md:order-1 order-3">© ICAROS Lab 2021</p><p class="hidden md:inline px-1 order-2">|</p><p class="w-full md:w-auto md:order-3 order-5">In memory of <a href="https://www.linkedin.com/in/jignesh-modi-7819a8b8">Jignesh</a></p><p class="hidden md:inline px-1 order-4">|</p><p class="w-full md:w-auto mb-1 md:mb-0 md:order-5 order-1">See anything wrong? <a href="https://github.com/icaros-usc/icaros-usc.github.io/issues/new/">Raise an Issue</a></p></div></div></div></footer><script src="/assets/main.js?e49202d1526e360b0fe7399dd607d56b"></script><script src="/assets/vendor.js?40d6fac326e5360678edc8ddd7f519b8"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.6/dist/clipboard.min.js"></script><script>new ClipboardJS(".clipboard")</script></body></html>